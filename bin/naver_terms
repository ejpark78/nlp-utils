#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import textwrap
from os import getenv

from crawler.naver_terms.core import TermsCore
from crawler.naver_terms.detail import TermsDetail
from crawler.naver_terms.list import TermsList

description = """검색팀 네이버 백과사전 크롤러"""

epilog = """\
usage example:

* 표제어 목록 수집
    PYTHONPATH=. \\
    CRAWLER_CONFIG=../config/naver-terms.yaml \\
    ELASTIC_SEARCH_INDEX=detail \\
    ELASTIC_SEARCH_LIST_INDEX=list \\
        bin/naver_terms \\
            --list \\
            --db-type sqlite \\
            --cache naver-terms.db            

* 본문 수집
    PYTHONPATH=. \\
    ELASTIC_SEARCH_INDEX=detail \\
    ELASTIC_SEARCH_LIST_INDEX=list \\
        bin/naver_terms \\
            --detail \\
            --db-type sqlite \\
            --cache naver-terms.db            

* 코퍼스 덤프
    PYTHONPATH=. \\
    bin/naver_terms \\
        --dump \\
        --db-type sqlite \\
        --cache naver-terms.db \\
        --config ../config/naver-terms.yaml \\
        --index detail \\
        --list-index list
"""


class TermsCrawler(object):
    """백과사전 크롤링"""

    def __init__(self):
        super().__init__()

    def batch(self) -> None:
        params = self.init_arguments()

        if params['list'] is True:
            TermsList(params=params).batch()

        if params['detail'] is True:
            TermsDetail(params=params).batch()

        if params['dump'] is True:
            TermsCore(params=params).dump()

        return

    @staticmethod
    def init_arguments() -> dict:
        global description, epilog

        parser = argparse.ArgumentParser(
            description=textwrap.dedent(description),
            epilog=textwrap.dedent(epilog),
            formatter_class=argparse.RawDescriptionHelpFormatter
        )

        # crawler options
        parser.add_argument('--list', action='store_true', default=False, help='목록')
        parser.add_argument('--detail', action='store_true', default=False, help='상세 정보')

        parser.add_argument('--dump', action='store_true', default=False)

        # crawler settings
        parser.add_argument('--sleep', default=getenv('CRAWLER_SLEEP', default=10), type=float, help='sleep time')
        parser.add_argument('--config', default=getenv('CRAWLER_CONFIG', default=None), type=str, help='설정 파일 정보')
        parser.add_argument('--sub-category', default=getenv('CRAWLER_SUBCATEGORY', default=''), help='하위 카테고리')

        # list options
        parser.add_argument('--list-start', default=getenv('CRAWLER_LIST_START', default=1), type=int, help='목록 시작 페이지')
        parser.add_argument('--list-end', default=getenv('CRAWLER_LIST_END', default=100000), type=int,
                            help='목록 종료 페이지')
        parser.add_argument('--list-step', default=getenv('CRAWLER_LIST_STEP', default=1), type=int, help='목록 스템 페이지')

        # detail options
        parser.add_argument('--size', default=getenv('CRAWLER_FETCH_SIZE', default=100), type=int, help='list fetch size')

        # selenium settings
        # parser.add_argument('--login', action='store_true', default=False)
        parser.add_argument('--head', action='store_true', default=False)
        parser.add_argument('--user-data', default=getenv('CRAWLER_SELENIUM_DATA_PATH', default=None))
        parser.add_argument('--executable-path', default=getenv('CRAWLER_CHROME_EXEC', default='/usr/bin/google-chrome'))

        # corpus lake type
        parser.add_argument('--db-type', default=getenv('CRAWLER_DB_TYPE', default='sqlite'), type=str,
                            help='sqlite or elasticsearch')

        # elasticsearch settings
        parser.add_argument('--host', default=getenv('ELASTIC_SEARCH_HOST', default=None), type=str,
                            help='elasticsearch url')
        parser.add_argument('--index', default=getenv('ELASTIC_SEARCH_INDEX', default=None), type=str,
                            help='elasticsearch index')
        parser.add_argument('--list-index', default=getenv('ELASTIC_SEARCH_LIST_INDEX', default=None), type=str,
                            help='elasticsearch list index')
        parser.add_argument('--auth', default=getenv('ELASTIC_SEARCH_AUTH', default=None), type=str,
                            help='elasticsearch auth')
        parser.add_argument('--auth-encoded', default=getenv('ELASTIC_SEARCH_AUTH_ENCODED', default=None), type=str,
                            help='elasticsearch auth')

        # cache
        parser.add_argument('--cache', default=getenv('CRAWLER_CACHE', default=None), type=str,
                            help='sqlite cache file name')

        return vars(parser.parse_args())


if __name__ == '__main__':
    TermsCrawler().batch()
