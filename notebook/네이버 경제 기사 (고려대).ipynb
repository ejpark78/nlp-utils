{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T04:08:41.473420Z",
     "start_time": "2019-12-02T04:08:41.458518Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T04:08:48.909184Z",
     "start_time": "2019-12-02T04:08:41.753413Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workspace/data-center/utils/elasticsearch_utils.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.elasticsearch_utils import ElasticSearchUtils\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "\n",
    "np.random.seed(0)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T04:08:48.941477Z",
     "start_time": "2019-12-02T04:08:48.912837Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    import re\n",
    "    \n",
    "    text = re.sub(r'/([A-Z]+?)[+]', '/\\g<1> ', text)\n",
    "    \n",
    "    result = []\n",
    "    for word in text.split(' '):\n",
    "        try:\n",
    "            w, p = word.rsplit('/', maxsplit=1)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if len(p) == 0:\n",
    "            continue\n",
    "\n",
    "        if p[0] == 'N':\n",
    "            result.append(w)    \n",
    "    \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T04:08:49.172798Z",
     "start_time": "2019-12-02T04:08:48.942718Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_doc(doc_list):\n",
    "    result = []\n",
    "    \n",
    "    for doc in tqdm(doc_list):\n",
    "        if 'nlu_wrapper' not in doc:\n",
    "            continue\n",
    "        \n",
    "        buf = []\n",
    "        for k in doc['nlu_wrapper']:\n",
    "            for item in doc['nlu_wrapper'][k]:\n",
    "                buf += item.values()\n",
    "        \n",
    "        str_buf = '\\n'.join(buf)\n",
    "        \n",
    "        result.append({\n",
    "            'document_id': doc['document_id'],\n",
    "            'date': doc['date'],                \n",
    "            'morp': str_buf,\n",
    "            'token': tokenizer(str_buf.replace('\\n', ' ')),\n",
    "        })\n",
    "                    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T04:08:49.274837Z",
     "start_time": "2019-12-02T04:08:49.178593Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def dump_docs(index):\n",
    "    host_info = {\n",
    "        'host': 'https://corpus.ncsoft.com:9200',\n",
    "        'http_auth': 'elastic:nlplab',\n",
    "    }\n",
    "\n",
    "    utils = ElasticSearchUtils(**host_info)\n",
    "    \n",
    "    query = {\n",
    "      '_source': [\n",
    "        'document_id',\n",
    "        'date',\n",
    "        'nlu_wrapper.*.morp_str',\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    doc_list = []\n",
    "    utils.export(index=index, query=query, result=doc_list)    \n",
    "    \n",
    "    nlu_wrapper = split_doc(doc_list)\n",
    "    \n",
    "    df = pd.DataFrame(nlu_wrapper)\n",
    "\n",
    "    df.fillna('', inplace=True)\n",
    "    \n",
    "    with open('data/{}.json.bz2'.format(index), 'w') as fp:\n",
    "        for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            line = json.dumps(dict(row), ensure_ascii=False)\n",
    "            fp.write(line + '\\n')    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T04:08:49.436431Z",
     "start_time": "2019-12-02T04:08:49.276138Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def read_docs(index):\n",
    "    with open('data/{}.json.bz2'.format(index), 'r') as fp:\n",
    "        doc_list = []\n",
    "        for line in tqdm(fp.readlines()):\n",
    "            doc = json.loads(line)\n",
    "            doc_list.append(doc)\n",
    "\n",
    "        df = pd.DataFrame(doc_list)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T03:52:31.364742Z",
     "start_time": "2019-12-02T03:52:31.328029Z"
    }
   },
   "outputs": [],
   "source": [
    "index_list = [\n",
    "#     'corpus_process-naver-economy-2010',\n",
    "#     'corpus_process-naver-economy-2011',\n",
    "#     'corpus_process-naver-economy-2012',\n",
    "#     'corpus_process-naver-economy-2013',\n",
    "#     'corpus_process-naver-economy-2014',\n",
    "#     'corpus_process-naver-economy-2015',\n",
    "#     'corpus_process-naver-economy-2016',\n",
    "#     'corpus_process-naver-economy-2017',\n",
    "#     'corpus_process-naver-economy-2018',\n",
    "    'corpus_process-naver-economy-2019',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T02:49:14.465962Z",
     "start_time": "2019-12-02T00:06:41.556120Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in tqdm(list(reversed(index_list))):\n",
    "    dump_docs(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T04:09:00.112956Z",
     "start_time": "2019-12-02T04:09:00.098483Z"
    }
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    use_idf=True,\n",
    "    ngram_range=(1, 3),\n",
    "    sublinear_tf=True,    # tf값에 1+log(tf)를 적용하여 tf값이 무한정 커지는 것을 막음\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T04:09:03.464761Z",
     "start_time": "2019-12-02T04:09:03.454413Z"
    }
   },
   "outputs": [],
   "source": [
    "index_list = [\n",
    "    'corpus_process-naver-economy-2010',\n",
    "    'corpus_process-naver-economy-2011',\n",
    "    'corpus_process-naver-economy-2012',\n",
    "    'corpus_process-naver-economy-2013',\n",
    "    'corpus_process-naver-economy-2014',\n",
    "    'corpus_process-naver-economy-2015',\n",
    "    'corpus_process-naver-economy-2016',\n",
    "    'corpus_process-naver-economy-2017',\n",
    "    'corpus_process-naver-economy-2018',\n",
    "    'corpus_process-naver-economy-2019',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T04:16:03.228804Z",
     "start_time": "2019-12-02T04:11:21.439102Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/465912 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 8563/465912 [00:00<00:05, 85625.39it/s]\u001b[A\n",
      "  4%|▍         | 20491/465912 [00:00<00:04, 93542.27it/s]\u001b[A\n",
      "  7%|▋         | 32390/465912 [00:00<00:04, 99954.10it/s]\u001b[A\n",
      " 10%|▉         | 44323/465912 [00:00<00:04, 105071.92it/s]\u001b[A\n",
      " 12%|█▏        | 56460/465912 [00:00<00:03, 109481.28it/s]\u001b[A\n",
      " 15%|█▍        | 68780/465912 [00:00<00:03, 113262.43it/s]\u001b[A\n",
      " 17%|█▋        | 80630/465912 [00:00<00:03, 114784.31it/s]\u001b[A\n",
      " 20%|█▉        | 92884/465912 [00:00<00:03, 117005.57it/s]\u001b[A\n",
      " 22%|██▏       | 104708/465912 [00:00<00:03, 117372.47it/s]\u001b[A\n",
      " 25%|██▍       | 116211/465912 [00:01<00:02, 116658.79it/s]\u001b[A\n",
      " 27%|██▋       | 127690/465912 [00:01<00:02, 115869.97it/s]\u001b[A\n",
      " 30%|██▉       | 139148/465912 [00:01<00:02, 114934.05it/s]\u001b[A\n",
      " 32%|███▏      | 150677/465912 [00:01<00:02, 115006.82it/s]\u001b[A\n",
      " 35%|███▍      | 162612/465912 [00:01<00:02, 116275.36it/s]\u001b[A\n",
      " 37%|███▋      | 174685/465912 [00:01<00:02, 117576.55it/s]\u001b[A\n",
      " 40%|████      | 186783/465912 [00:01<00:02, 118576.93it/s]\u001b[A\n",
      " 43%|████▎     | 199173/465912 [00:01<00:02, 120122.31it/s]\u001b[A\n",
      " 45%|████▌     | 211394/465912 [00:01<00:02, 120741.04it/s]\u001b[A\n",
      " 48%|████▊     | 223465/465912 [00:01<00:02, 119854.53it/s]\u001b[A\n",
      " 51%|█████     | 235450/465912 [00:02<00:01, 117961.52it/s]\u001b[A\n",
      " 53%|█████▎    | 247466/465912 [00:02<00:01, 118611.68it/s]\u001b[A\n",
      " 56%|█████▌    | 259803/465912 [00:02<00:01, 119998.80it/s]\u001b[A\n",
      " 58%|█████▊    | 271811/465912 [00:02<00:01, 117131.50it/s]\u001b[A\n",
      " 61%|██████    | 283545/465912 [00:02<00:01, 116029.11it/s]\u001b[A\n",
      " 63%|██████▎   | 295165/465912 [00:02<00:01, 114750.86it/s]\u001b[A\n",
      " 66%|██████▌   | 306655/465912 [00:02<00:01, 113705.00it/s]\u001b[A\n",
      " 68%|██████▊   | 318375/465912 [00:02<00:01, 114730.32it/s]\u001b[A\n",
      " 71%|███████   | 329985/465912 [00:02<00:01, 115137.61it/s]\u001b[A\n",
      " 73%|███████▎  | 342210/465912 [00:02<00:01, 117182.11it/s]\u001b[A\n",
      " 76%|███████▌  | 353943/465912 [00:03<00:00, 116298.55it/s]\u001b[A\n",
      " 79%|███████▊  | 366209/465912 [00:03<00:00, 118134.93it/s]\u001b[A\n",
      " 81%|████████  | 378483/465912 [00:03<00:00, 119479.04it/s]\u001b[A\n",
      " 84%|████████▍ | 390757/465912 [00:03<00:00, 120437.33it/s]\u001b[A\n",
      " 86%|████████▋ | 402820/465912 [00:03<00:00, 120494.41it/s]\u001b[A\n",
      " 89%|████████▉ | 414930/465912 [00:03<00:00, 120674.45it/s]\u001b[A\n",
      " 92%|█████████▏| 427004/465912 [00:03<00:00, 117481.50it/s]\u001b[A\n",
      " 94%|█████████▍| 438775/465912 [00:03<00:00, 117075.58it/s]\u001b[A\n",
      " 97%|█████████▋| 450499/465912 [00:03<00:00, 116418.72it/s]\u001b[A\n",
      "100%|██████████| 465912/465912 [00:03<00:00, 117125.64it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8774342b01b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \"\"\"\n\u001b[1;32m   1630\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mn_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_sort_features\u001b[0;34m(self, X, vocabulary)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mreordered\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodifies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \"\"\"\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0msorted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m         \u001b[0mmap_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index in tqdm(index_list):\n",
    "    df = read_docs(index=index)\n",
    "    \n",
    "    vec.fit(df['token'].to_list())\n",
    "    \n",
    "    with open('data/{}.tfidf.csv'.format(index), 'w') as fp:\n",
    "        idfs = vec.idf_\n",
    "        \n",
    "        for i, f in enumerate(vec.get_feature_names()):\n",
    "            fp.write('{feature}\\t{idf}\\n'.format(feature=f, idf=idfs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(vec.get_feature_names(), vec.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-02T04:07:23.975Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(vec.idf_, index=vec.get_feature_names(), columns=['tfidf'])\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-02T04:07:26.337Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df.sort_values('tfidf', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-02T04:07:26.696Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df.to_csv('data/{}-(2018~2019).csv'.format(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T04:02:29.861529Z",
     "start_time": "2019-11-29T04:02:29.847120Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XeCBh3UzZhE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
